Language Model Post Training Quantization (PTQ) Using Quark
===========================================================

This document provides examples of post training quantizing (PTQ) and exporting the language models (OPT, Llama…) using Quark. For evaluation of quantized model, please refer to :doc:`Model Evaluation <example_quark_torch_llm_eval>`.


Table of Contents
=================

.. contents::
  :local:
  :depth: 1

Supported Models
----------------

+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| Model Name                                 | FP8①| INT②  | MX③  | AWQ/GPTQ(INT)④  | SmoothQuant | Rotation |
+============================================+=====+=======+======+=================+=============+==========+
| meta-llama/Llama-2-*-hf ⑤                  | ✓   | ✓     | ✓    | ✓               | ✓           | ✓        |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| meta-llama/Llama-3-*B(-Instruct)           | ✓   | ✓     | ✓    | ✓               | ✓           | ✓        |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| meta-llama/Llama-3.1-*B(-Instruct)         | ✓   | ✓     | ✓    | ✓               | ✓           | ✓        |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| meta-llama/Llama-3.2-*B(-Instruct)         | ✓   | ✓     | ✓    | ✓               | ✓           | ✓        |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| meta-llama/Llama-3.2-*B-Vision(-Instruct) ⑥| ✓   | ✓     |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| facebook/opt-*                             | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| EleutherAI/gpt-j-6b                        | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| THUDM/chatglm3-6b                          | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| Qwen/Qwen-*                                | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| Qwen/Qwen1.5-*                             | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| Qwen/Qwen1.5-MoE-A2.7B                     | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| Qwen/Qwen2-*                               | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| microsoft/phi-2                            | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| microsoft/Phi-3-mini-*k-instruct           | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| microsoft/Phi-3.5-mini-instruct            | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| mistralai/Mistral-7B-v0.1                  | ✓   | ✓     | ✓    | ✓               | ✓           |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| mistralai/Mixtral-8x7B-v0.1                | ✓   | ✓     |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| hpcai-tech/grok-1                          | ✓   | ✓     |      | ✓               |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| CohereForAI/c4ai-command-r-plus-08-2024    | ✓   |       |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| CohereForAI/c4ai-command-r-08-2024         | ✓   |       |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| CohereForAI/c4ai-command-r-plus            | ✓   |       |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| CohereForAI/c4ai-command-r-v01             | ✓   |       |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| databricks/dbrx-instruct                   | ✓   |       |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+
| deepseek-ai/deepseek-moe-16b-chat          | ✓   |       |      |                 |             |          |
+--------------------------------------------+-----+-------+------+-----------------+-------------+----------+


.. note::
   - ① FP8 means ``OCP fp8_e4m3`` data type quantization.
   - ② INT includes INT8, UINT8, INT4, UINT4 data type quantization
   - ③ MX includes OCP data type MXINT8, MXFP8E4M3, MXFP8E5M2, MXFP4, MXFP6E3M2, MXFP6E2M3.
   - ④ GPTQ only supports QuantScheme as 'PerGroup' and 'PerChannel'.
   - ⑤ ``*`` represents different model sizes, such as ``7b``.
   - ⑥ meta-llama/Llama-3.2-*B-Vision models only quantize language parts.

How to get the example code and script
-----------------------

Users can get the example code after downloading and unzipping ``quark.zip`` (referring to :doc:`Installation Guide <install>`).
The example folder is in quark.zip.

   Directory Structure of the ZIP File:

   ::

         + quark.zip
            + examples
               + torch
                  + language_modeling
                     + llm_ptq
                        + README.md                       <--- Scripts and Recipes
                        + quantize_quark.py               <--- Main function of example
                        + configuration_preparation.py
                     + utils
                        + data_preparation.py
                        + model_preparation.py

.. raw:: html

   <!--
   ## License
   Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved. SPDX-License-Identifier: MIT
   -->
